{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Report of IHOPEFINALTHIS.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ItuXsbe-Xnw5",
        "8CkBnuGeZSoL",
        "euoe8NwtelTK",
        "CFRYd9UTIaFF",
        "77MJWF24Jfpu",
        "xXRzzskYMZC2",
        "tlEq0OsVOMbW",
        "yn6s-JMOQIKu"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRxhpW8vcvar",
        "colab_type": "text"
      },
      "source": [
        "# **\"Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts\"**\n",
        "\n",
        "*Reproduced by Kassymkhan Tengel and Nagima Chalkarova*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1fU6eQGc7Dg",
        "colab_type": "text"
      },
      "source": [
        "# **Introduction**\n",
        "---\n",
        "\n",
        "   Nowadays the number of different data is increasing. Especially, there is an exponential growth in textual data. That is why a big amount of studies is actively conducted for the text analysis. Most of works are related to the classification of texts. Because textual data can have various categories, such as author gender, sentiment, language category and so on. In this work sentiment analysis is studied. It involves classes such as very negative, negative, neutral, positive, and very positive. The sentiment analysis of short texts is important. It has its own applications in different industrial fields. For example, on the websites about restaurants or movies, people can leave their opinions or sentiments regarding some places or movies, respectively. If these sentiments are well predicted, then it can be used further to give restaurants or movies recommendations and so on. \n",
        "\n",
        "   This work is aimed to reproduce the paper of Santos and Gatti  called “Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts” [1](https://www.aclweb.org/anthology/C14-1008.pdf). In their work, the aim is to do sentiment classification of short texts like Twitter messages which is challenging task. Because it usually contains limited contextual information. They used a deep convolutional network. The proposed network is called Character to Sentence Convolutional Neural Network (CharSCNN).The strenght of this network is that it analyzes the data in character level, in other words more deeper than others. It uses two convolutional layers. .One is aimed to extract related features from words and another is used to extract related features from sentences. The next model is called SCNN, which stands for Sentence Convolutional Neural Networks. It is in some way similar to previous model, but the difference is that it dpes not analyze the data in character level, just in sentence level. It is assumed that the first one is more accurate than latter. Also, the paper shows that using unsupervised pre-training is useful and compares different methods of machine learning for classification with these proposed models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q22P32TSoy5M",
        "colab_type": "text"
      },
      "source": [
        "# **Data description** \n",
        "---\n",
        "Two datasets were used in this paper. They are Twitter posts, which are from Stanford Twitter Sentiment (STS) corpus and movie reviews, which are proposed by Stanford Sentiment Treebank (SSTb). STS corpus has 1.6 million Twitter messages with class labels positive or negative. In the experiment we took randomly 80,000 tweets for training set and 20% of training set (16,000) was taken for validation set. Test set which was manually picked consists of 498 tweets. SSTb corpus contains fine grained (5 classes) sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences. Training set contains 8544 sentences, validation set contains 1101 sentences and test set has 2210 sentences [1](https://www.aclweb.org/anthology/C14-1008.pdf).\n",
        "\n",
        "Dataset | Set | # of sentences/tweets | # of classes\n",
        "--- | --- | --- | ---\n",
        "             | Training | 8544 | 5\n",
        "SSTb | Validation | 1101 | 5\n",
        "             | Test| 2210 | 5\n",
        "             | Training | 80,000 | 2\n",
        "STS | Validation | 16,000 | 2\n",
        "             | Test | 498 | 2\n",
        "             |**Table-1. Dataset for Sentiment analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItuXsbe-Xnw5",
        "colab_type": "text"
      },
      "source": [
        "# **Necessary Python libraries**\n",
        "---\n",
        "The code below contains importings of necessary python tools. Most tools are from keras. And we use google colab to get data from my drive. Also, the constants which will be used throgout the project are given."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ws5QSZnEeqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import sys\n",
        "from numpy import loadtxt\n",
        "import os\n",
        "from numpy import array, asarray, zeros\n",
        "from keras.preprocessing.text import one_hot, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SpatialDropout1D, LSTM\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
        "from keras.models import Model\n",
        "adam = Adam(lr=0.01)\n",
        "num_epochs = 10\n",
        "val_portion=0.2\n",
        "embedding_dim = 100\n",
        "training_size=80000\n",
        "max_length = 20\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_qDo-LOGB_v",
        "colab_type": "text"
      },
      "source": [
        "# **Reproduction for STS dataset**\n",
        "---\n",
        "Before working with models, it is better to preprocess the dataset. Our data has 6 columns, but 4 of them are not necesssary. So we deleted them, and 2 remaining columns containins the label and and tweet were used. After we removed unnecessary columns, we shuffled our data with two columns. Then by using tokenizer from Keras, we set tokens to each sentences, then by using padded we made our sentences have the same lenght. This process was applied to train data, val data and test data. Now, we can work with models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CkBnuGeZSoL",
        "colab_type": "text"
      },
      "source": [
        "# Pretrained SCNN model\n",
        "The code below contains the pretrained SCNN model for STS dataset. To implement the model We used only some portion of STS dataset as author did. He used 80K samples for training data. Also, from 80k samples, its 20% is used for validation data. For the test portion, We manually picked 498 another samples. SCNN model contains one embedding layer, 2 bidirectional layers and 2 dense layers. For the embedding layer I used fixed valued elements such as vocab size, input size and embedding dimension. Also, the weights were derived from pretrained 100 dimensional glove file with 6 billion tokens inside. It is an open source material that can be used by everyone. The first LSTM bidirectional layer has 64 neurons, whereas the latter LSTM has 32 neurons. Actually, it all depends on user, but preferably, it is better to use values that can bring high accuracy of the model. The first dense layer has relu activation, whereas the next one has sigmoid activation function. Sigmoid is used, because our data contains binary labels. As optimizer, Adam optimizer with 0.01 learning rate was chosen. We decided to run epochs as author did, because increasing epoch number increases the time to run. As a resutl, the accuracy obtained from model reached 77%. When model was evaluated with test data, it reached almost 80% accuracy. It is worth to mention that we tried many models such as GRU, single LSTM, Convolution in 1D, but the best one among them was bidirectional lstm model. If to compare with author's accuracy which is 85.2%, our model did it poorly. However, it is due to some differences in our models. Author used wikipedia of 2013th year as pretrained data, but we could not find it, so instead of it we used 100 dimensional glove tokens. STS dataset was trained with other models. The following results were obtained: SVM-82.2%, NB-82.7%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K28fWEHMzK7D",
        "colab_type": "code",
        "outputId": "8207a7d3-9dc8-446a-fc18-9309d31c85cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        }
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1pIQYABUucCVV7qKy4sl8EWZfiBaNzYMu' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1pIQYABUucCVV7qKy4sl8EWZfiBaNzYMu\" -O /content/tootrain.csv && rm -rf /tmp/cookies.txt\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1xoJwRR_1nnGFeBtngzbp7SLDhxc4ijSJ' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1xoJwRR_1nnGFeBtngzbp7SLDhxc4ijSJ\" -O /content/totest.csv && rm -rf /tmp/cookies.txt\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1R-V-TP8TkQQLTcQVEVSagIU_U6N4_jth' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1R-V-TP8TkQQLTcQVEVSagIU_U6N4_jth\" -O /content/glove.6B.100d.txt && rm -rf /tmp/cookies.txt\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-22 11:10:03--  https://docs.google.com/uc?export=download&confirm=ko9B&id=1pIQYABUucCVV7qKy4sl8EWZfiBaNzYMu\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.125.101, 108.177.125.138, 108.177.125.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.125.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0g-3s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/l8ilscfubh99cflbjkufdrtp640sn969/1574416800000/14221645452890223212/*/1pIQYABUucCVV7qKy4sl8EWZfiBaNzYMu?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2019-11-22 11:10:03--  https://doc-0g-3s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/l8ilscfubh99cflbjkufdrtp640sn969/1574416800000/14221645452890223212/*/1pIQYABUucCVV7qKy4sl8EWZfiBaNzYMu?e=download\n",
            "Resolving doc-0g-3s-docs.googleusercontent.com (doc-0g-3s-docs.googleusercontent.com)... 64.233.189.132, 2404:6800:4008:c07::84\n",
            "Connecting to doc-0g-3s-docs.googleusercontent.com (doc-0g-3s-docs.googleusercontent.com)|64.233.189.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘/content/tootrain.csv’\n",
            "\n",
            "/content/tootrain.c     [             <=>    ] 227.74M  61.2MB/s    in 3.7s    \n",
            "\n",
            "2019-11-22 11:10:07 (61.2 MB/s) - ‘/content/tootrain.csv’ saved [238803811]\n",
            "\n",
            "--2019-11-22 11:10:10--  https://docs.google.com/uc?export=download&confirm=&id=1xoJwRR_1nnGFeBtngzbp7SLDhxc4ijSJ\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.125.113, 108.177.125.100, 108.177.125.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.125.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0g-3s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/it463fa88ahcrjlr81e738js8s3868ia/1574416800000/14221645452890223212/*/1xoJwRR_1nnGFeBtngzbp7SLDhxc4ijSJ?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2019-11-22 11:10:11--  https://doc-0g-3s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/it463fa88ahcrjlr81e738js8s3868ia/1574416800000/14221645452890223212/*/1xoJwRR_1nnGFeBtngzbp7SLDhxc4ijSJ?e=download\n",
            "Resolving doc-0g-3s-docs.googleusercontent.com (doc-0g-3s-docs.googleusercontent.com)... 64.233.189.132, 2404:6800:4008:c07::84\n",
            "Connecting to doc-0g-3s-docs.googleusercontent.com (doc-0g-3s-docs.googleusercontent.com)|64.233.189.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 74326 (73K) [text/csv]\n",
            "Saving to: ‘/content/totest.csv’\n",
            "\n",
            "/content/totest.csv 100%[===================>]  72.58K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2019-11-22 11:10:11 (122 MB/s) - ‘/content/totest.csv’ saved [74326/74326]\n",
            "\n",
            "--2019-11-22 11:10:13--  https://docs.google.com/uc?export=download&confirm=fVji&id=1R-V-TP8TkQQLTcQVEVSagIU_U6N4_jth\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.125.100, 108.177.125.113, 108.177.125.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.125.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0k-3s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/1dli3a1amu2s0deg69r4ujo9fmi0v5vi/1574416800000/14221645452890223212/*/1R-V-TP8TkQQLTcQVEVSagIU_U6N4_jth?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2019-11-22 11:10:13--  https://doc-0k-3s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/1dli3a1amu2s0deg69r4ujo9fmi0v5vi/1574416800000/14221645452890223212/*/1R-V-TP8TkQQLTcQVEVSagIU_U6N4_jth?e=download\n",
            "Resolving doc-0k-3s-docs.googleusercontent.com (doc-0k-3s-docs.googleusercontent.com)... 64.233.189.132, 2404:6800:4008:c07::84\n",
            "Connecting to doc-0k-3s-docs.googleusercontent.com (doc-0k-3s-docs.googleusercontent.com)|64.233.189.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/plain]\n",
            "Saving to: ‘/content/glove.6B.100d.txt’\n",
            "\n",
            "/content/glove.6B.1     [                 <=>] 331.04M  52.1MB/s    in 6.3s    \n",
            "\n",
            "2019-11-22 11:10:20 (52.1 MB/s) - ‘/content/glove.6B.100d.txt’ saved [347116733]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Fc-rjgNFKX0",
        "colab_type": "code",
        "outputId": "7fd327b0-1a64-46de-dce9-3fcec1cf3d26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        }
      },
      "source": [
        "corpus = []\n",
        "sentences=[]\n",
        "labels=[]\n",
        "num_sentences = 0\n",
        "\n",
        "with open('/content/tootrain.csv',encoding='cp1252') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in reader:\n",
        "        list_item=[]\n",
        "        list_item.append(row[5])\n",
        "        this_label=row[0]\n",
        "        if this_label=='0':\n",
        "            list_item.append(0)\n",
        "        else:\n",
        "            list_item.append(1)\n",
        "        num_sentences = num_sentences + 1\n",
        "        corpus.append(list_item)\n",
        "random.shuffle(corpus)\n",
        "for x in range(training_size):\n",
        "    sentences.append(corpus[x][0])\n",
        "    labels.append(corpus[x][1])\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size=len(word_index)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "split = int(val_portion * training_size)\n",
        "test_sequences = padded[0:split]\n",
        "training_sequences = padded[split:training_size]\n",
        "test_labels = labels[0:split]\n",
        "training_labels = labels[split:training_size]\n",
        "embeddings_index = {};\n",
        "with open('/content/glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split();\n",
        "        word = values[0];\n",
        "        coefs = np.asarray(values[1:], dtype='float32');\n",
        "        embeddings_index[word] = coefs;\n",
        "\n",
        "embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word);\n",
        "    if embedding_vector is not None:\n",
        "        embeddings_matrix[i] = embedding_vector;\n",
        "# model = tf.keras.Sequential([\n",
        "# tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n",
        "# tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
        "# tf.keras.layers.MaxPooling1D(pool_size=4),\n",
        "# tf.keras.layers.Flatten(),\n",
        "# tf.keras.layers.Dropout(0.2),\n",
        "# tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "# ])\n",
        "# model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length,weights=[embeddings_matrix], trainable=False),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n",
        "history1 = model.fit(training_sequences, training_labels, epochs=num_epochs, validation_data=(test_sequences, test_labels), verbose=1)   \n",
        "newcorpus = []\n",
        "newsentences=[]\n",
        "newlabels=[]\n",
        "with open('/content/totest.csv',encoding='cp1252') as csvfile:\n",
        "    newreader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in newreader:\n",
        "        list_item=[]\n",
        "        list_item.append(row[5])\n",
        "        this_label=row[0]\n",
        "        if this_label=='0':\n",
        "            list_item.append(0)\n",
        "        else:\n",
        "            list_item.append(1)\n",
        "        num_sentences = num_sentences + 1\n",
        "        newcorpus.append(list_item)\n",
        "random.shuffle(newcorpus)\n",
        "for x in range(s):\n",
        "    newsentences.append(newcorpus[x][0])\n",
        "    newlabels.append(newcorpus[x][1])\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(newsentences)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size=len(word_index)\n",
        "sequences1 = tokenizer.texts_to_sequences(newsentences)\n",
        "padded1 = pad_sequences(sequences1, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "teest_sequences = padded[0:498]\n",
        "teest_labels = labels[0:498]\n",
        "results1 = model.evaluate(teest_sequences, teest_labels, batch_size=128)                   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 20, 100)           8438900   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 20, 128)           84480     \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 64)                41216     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 390       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 7         \n",
            "=================================================================\n",
            "Total params: 8,564,993\n",
            "Trainable params: 126,093\n",
            "Non-trainable params: 8,438,900\n",
            "_________________________________________________________________\n",
            "Train on 64000 samples, validate on 16000 samples\n",
            "Epoch 1/10\n",
            "64000/64000 [==============================] - 291s 5ms/sample - loss: 0.5528 - acc: 0.7132 - val_loss: 0.5134 - val_acc: 0.7453\n",
            "Epoch 2/10\n",
            "64000/64000 [==============================] - 290s 5ms/sample - loss: 0.4971 - acc: 0.7569 - val_loss: 0.4960 - val_acc: 0.7564\n",
            "Epoch 3/10\n",
            "57760/64000 [==========================>...] - ETA: 26s - loss: 0.4701 - acc: 0.7750"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euoe8NwtelTK",
        "colab_type": "text"
      },
      "source": [
        "# Random SCNN model\n",
        "The model below consists of series of layers such as embedding layer, bidirectional lstm layers, dense layers. Each layer has its own characteristics which can be seen in the code. The data proportion is same as in the previous model. We tried many different combination of models,but the peak of our accuracy was almost 70%. For the unused test data, accuracy was 71%. For the same model with slightly differences, author had 82.2%, SVM model had 82.2% and NB model had 82.7% accuracy.The reason for that is SCNN model is a some convolution model that does not consist of concrete layers, so we tried many combinations of models, but the peak was reached by used model. Actually, we had the same trend in the reproduction. As author, with pretrained model we had greater accuracy than non trained random model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QTTGST3G081",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "sentences=[]\n",
        "labels=[]\n",
        "num_sentences = 0\n",
        "\n",
        "with open('/content/tootrain.csv',encoding='cp1252') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in reader:\n",
        "        list_item=[]\n",
        "        list_item.append(row[5])\n",
        "        this_label=row[0]\n",
        "        if this_label=='0':\n",
        "            list_item.append(0)\n",
        "        else:\n",
        "            list_item.append(1)\n",
        "        num_sentences = num_sentences + 1\n",
        "        corpus.append(list_item)\n",
        "random.shuffle(corpus)\n",
        "for x in range(training_size):\n",
        "    sentences.append(corpus[x][0])\n",
        "    labels.append(corpus[x][1])\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size=len(word_index)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "split = int(val_portion * training_size)\n",
        "test_sequences = padded[0:split]\n",
        "training_sequences = padded[split:training_size]\n",
        "test_labels = labels[0:split]\n",
        "training_labels = labels[split:training_size]\n",
        "# model = tf.keras.Sequential([\n",
        "# tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n",
        "# tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
        "# tf.keras.layers.MaxPooling1D(pool_size=4),\n",
        "# tf.keras.layers.Flatten(),\n",
        "# tf.keras.layers.Dropout(0.2),\n",
        "# tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "# ])\n",
        "# model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, trainable=False),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n",
        "history2 = model.fit(training_sequences, training_labels, epochs=num_epochs, validation_data=(test_sequences, test_labels), verbose=1)   \n",
        "newcorpus = []\n",
        "newsentences=[]\n",
        "newlabels=[]\n",
        "with open('/content/totest.csv',encoding='cp1252') as csvfile:\n",
        "    newreader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in newreader:\n",
        "        list_item=[]\n",
        "        list_item.append(row[5])\n",
        "        this_label=row[0]\n",
        "        if this_label=='0':\n",
        "            list_item.append(0)\n",
        "        else:\n",
        "            list_item.append(1)\n",
        "        num_sentences = num_sentences + 1\n",
        "        newcorpus.append(list_item)\n",
        "random.shuffle(newcorpus)\n",
        "for x in range(s):\n",
        "    newsentences.append(newcorpus[x][0])\n",
        "    newlabels.append(newcorpus[x][1])\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(newsentences)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size=len(word_index)\n",
        "sequences1 = tokenizer.texts_to_sequences(newsentences)\n",
        "padded1 = pad_sequences(sequences1, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "teest_sequences = padded[0:498]\n",
        "teest_labels = labels[0:498]\n",
        "results2 = model.evaluate(teest_sequences, teest_labels, batch_size=128)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFRYd9UTIaFF",
        "colab_type": "text"
      },
      "source": [
        "# Pretrained CharSCNN model\n",
        "The following model was assumed to be more accurate, since it works with character level, and so it analyses the data more deeply. We enumerate each character with different numbers, and using this numbers convert the data into the array of numbers. Then by using the weights derived from pretrained glovo model, we construct our model and run it. The model has two 1D convolution layers with maxpoolings, flatten layer and dense layer. That list of layers is given by author. We use dropout element to drop some less significant terms. Activation is sigmoid and optimizer is adam as before. The model reached an accuracy of 76% which good enough. For the test data, unused 498 samples, accuracy was 72%. Author had 86.4% accuracy for this model. But it seems he had better pretrained data. Since our pretrained data is glove 100 dimension with 6 billion tokens, it may be non char level causing obstacles for our model, also lowering our accuracy. But, logically, char level cnn should have greater accuracy than SCNN model, since it analyses the data much more deeper. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc6gBroxHkv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "with open('/content/tootrain.csv',encoding='cp1252') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in reader:\n",
        "        list_item=[]\n",
        "        list_item.append(row[5])\n",
        "        this_label=row[0]\n",
        "        if this_label=='0':\n",
        "            list_item.append(0)\n",
        "        else:\n",
        "            list_item.append(1)\n",
        "        num_sentences = num_sentences + 1\n",
        "        corpus.append(list_item)\n",
        "sentences=[]\n",
        "labels=[]\n",
        "num_sentences = 0\n",
        "random.shuffle(corpus)\n",
        "for x in range(training_size):\n",
        "    sentences.append(corpus[x][0])\n",
        "    labels.append(corpus[x][1])\n",
        "split = int(val_portion * training_size)\n",
        "test_sequences = sentences[0:split]\n",
        "training_sequences = sentences[split:training_size]\n",
        "test_labels = labels[0:split]\n",
        "training_labels = labels[split:training_size]\n",
        "train_texts = training_sequences\n",
        "train_texts = [s.lower() for s in train_texts]\n",
        "\n",
        "\n",
        "test_texts = test_sequences\n",
        "test_texts = [s.lower() for s in test_texts]\n",
        "\n",
        "# =======================Convert string to index================\n",
        "# Tokenizer\n",
        "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "tk.fit_on_texts(train_texts)\n",
        "# If we already have a character list, then replace the tk.word_index\n",
        "# If not, just skip below part\n",
        "\n",
        "# -----------------------Skip part start--------------------------\n",
        "# construct a new vocabulary\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "\n",
        "# Use char_dict to replace the tk.word_index\n",
        "tk.word_index = char_dict.copy()\n",
        "# Add 'UNK' to the vocabulary\n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
        "# -----------------------Skip part end----------------------------\n",
        "\n",
        "# Convert string to index\n",
        "train_seq = tk.texts_to_sequences(train_texts)\n",
        "test_texts = tk.texts_to_sequences(test_texts)\n",
        "\n",
        "# Padding\n",
        "train_data = pad_sequences(train_seq, maxlen=1014, padding='post')\n",
        "test_data = pad_sequences(test_texts, maxlen=1014, padding='post')\n",
        "\n",
        "# Convert to numpy array\n",
        "train_data = np.array(train_data, dtype='float32')\n",
        "test_data = np.array(test_data, dtype='float32')\n",
        "input_size = 1014\n",
        "vocab_size = len(tk.word_index)\n",
        "embedding_size = 100\n",
        "\n",
        "fully_connected_layers = [1024, 1024]\n",
        "num_of_classes = 1\n",
        "dropout_p = 0.2\n",
        "optimizer = 'adam'\n",
        "loss = 'binary_crossentropy'\n",
        "\n",
        "embeddings_index = {};\n",
        "with open('/content/glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split();\n",
        "        word = values[0];\n",
        "        coefs = np.asarray(values[1:], dtype='float32');\n",
        "        embeddings_index[word] = coefs;\n",
        "\n",
        "embeddings_matrix = np.zeros((vocab_size+1, embedding_size));\n",
        "for word, i in tk.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word);\n",
        "    if embedding_vector is not None:\n",
        "        embeddings_matrix[i] = embedding_vector;\n",
        "import tensorflow as tf\n",
        "model = tf.keras.Sequential([\n",
        "tf.keras.layers.Embedding(vocab_size + 1,\n",
        "                            5,\n",
        "                            input_length=input_size,\n",
        "                            weights=[embeddings_matrix]),\n",
        "tf.keras.layers.Conv1D(256, 5, activation='relu'),\n",
        "tf.keras.layers.MaxPooling1D(pool_size=4),\n",
        "tf.keras.layers.Conv1D(256, 5, activation='relu'),\n",
        "tf.keras.layers.MaxPooling1D(pool_size=4),\n",
        "tf.keras.layers.Flatten(),\n",
        "tf.keras.layers.Dropout(0.2),\n",
        "tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])  # Adam, categorical_crossentropy\n",
        "model.summary()\n",
        "\n",
        "# Shuffle\n",
        "\n",
        "\n",
        "x_train = train_data\n",
        "y_train = training_labels\n",
        "\n",
        "x_test = test_data\n",
        "y_test = test_labels\n",
        "\n",
        "# Training\n",
        "history3 = model.fit(x_train, y_train,\n",
        "          validation_data=(x_test, y_test), \n",
        "          batch_size=128,\n",
        "          epochs=10,\n",
        "          verbose=1)  \n",
        "newcorpus = []\n",
        "newsentences=[]\n",
        "newlabels=[]\n",
        "with open('/content/totest.csv',encoding='cp1252') as csvfile:\n",
        "    newreader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in newreader:\n",
        "        list_item=[]\n",
        "        list_item.append(row[5])\n",
        "        this_label=row[0]\n",
        "        if this_label=='0':\n",
        "            list_item.append(0)\n",
        "        else:\n",
        "            list_item.append(1)\n",
        "        num_sentences = num_sentences + 1\n",
        "        newcorpus.append(list_item)\n",
        "    \n",
        "random.shuffle(newcorpus)\n",
        "sentences=[]\n",
        "labels=[]\n",
        "num_sentences = 0\n",
        "\n",
        "\n",
        "for x in range(s):\n",
        "    newsentences.append(newcorpus[x][0])\n",
        "    newlabels.append(newcorpus[x][1])\n",
        "\n",
        "\n",
        "test_sequences = newsentences[0:498]\n",
        "teest_labels = newlabels[0:498]  \n",
        "teest_texts = test_sequences\n",
        "teest_texts = [s.lower() for s in teest_texts]\n",
        "\n",
        "\n",
        "\n",
        "# =======================Convert string to index================\n",
        "# Tokenizer\n",
        "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "tk.fit_on_texts(teest_texts)\n",
        "# If we already have a character list, then replace the tk.word_index\n",
        "# If not, just skip below part\n",
        "\n",
        "# -----------------------Skip part start--------------------------\n",
        "# construct a new vocabulary\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "\n",
        "# Use char_dict to replace the tk.word_index\n",
        "tk.word_index = char_dict.copy()\n",
        "# Add 'UNK' to the vocabulary\n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
        "# -----------------------Skip part end----------------------------\n",
        "\n",
        "# Convert string to index\n",
        "teest_seq = tk.texts_to_sequences(teest_texts)\n",
        "\n",
        "# Padding\n",
        "teest_data = pad_sequences(teest_seq, maxlen=1014, padding='post')\n",
        "\n",
        "# Convert to numpy array\n",
        "teest_data = np.array(teest_data, dtype='float32')\n",
        "result3 = model.evaluate(teest_data, teest_labels, batch_size=128)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77MJWF24Jfpu",
        "colab_type": "text"
      },
      "source": [
        "# Random CharSCNN model\n",
        "This model is as previous model trains the data deeply with character level. The difference is that we do not use here pretrained glove tokens. Model as before consists of layers that author stated. Activation is sigmoid, optimizer is adam with learning rate 0.01. As we mentioned, it is logically true that char scnn model should have more accuracy than simple scnn model, it is due the deepness of the model. If in previous data we had 76% accuracy, here without pretrained information, we have 78% accuracy. It is one more proof of that glove token is not good pretrained data for charscnn, because instead of bettering the accuracy, it lowered the acccuracy. The authar had 82.2% accuracy for the same model, but our model is also good enough, if we compare it with previous 3 models.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HzNk-ynJfAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "with open('/content/tootrain.csv',encoding='cp1252') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in reader:\n",
        "        list_item=[]\n",
        "        list_item.append(row[5])\n",
        "        this_label=row[0]\n",
        "        if this_label=='0':\n",
        "            list_item.append(0)\n",
        "        else:\n",
        "            list_item.append(1)\n",
        "        num_sentences = num_sentences + 1\n",
        "        corpus.append(list_item)\n",
        "sentences=[]\n",
        "labels=[]\n",
        "num_sentences = 0\n",
        "random.shuffle(corpus)\n",
        "for x in range(training_size):\n",
        "    sentences.append(corpus[x][0])\n",
        "    labels.append(corpus[x][1])\n",
        "split = int(val_portion * training_size)\n",
        "test_sequences = sentences[0:split]\n",
        "training_sequences = sentences[split:training_size]\n",
        "test_labels = labels[0:split]\n",
        "training_labels = labels[split:training_size]\n",
        "train_texts = training_sequences\n",
        "train_texts = [s.lower() for s in train_texts]\n",
        "\n",
        "\n",
        "test_texts = test_sequences\n",
        "test_texts = [s.lower() for s in test_texts]\n",
        "\n",
        "# =======================Convert string to index================\n",
        "# Tokenizer\n",
        "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "tk.fit_on_texts(train_texts)\n",
        "# If we already have a character list, then replace the tk.word_index\n",
        "# If not, just skip below part\n",
        "\n",
        "# -----------------------Skip part start--------------------------\n",
        "# construct a new vocabulary\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "\n",
        "# Use char_dict to replace the tk.word_index\n",
        "tk.word_index = char_dict.copy()\n",
        "# Add 'UNK' to the vocabulary\n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
        "# -----------------------Skip part end----------------------------\n",
        "\n",
        "# Convert string to index\n",
        "train_seq = tk.texts_to_sequences(train_texts)\n",
        "test_texts = tk.texts_to_sequences(test_texts)\n",
        "\n",
        "# Padding\n",
        "train_data = pad_sequences(train_seq, maxlen=1014, padding='post')\n",
        "test_data = pad_sequences(test_texts, maxlen=1014, padding='post')\n",
        "\n",
        "# Convert to numpy array\n",
        "train_data = np.array(train_data, dtype='float32')\n",
        "test_data = np.array(test_data, dtype='float32')\n",
        "input_size = 1014\n",
        "vocab_size = len(tk.word_index)\n",
        "embedding_size = 100\n",
        "\n",
        "fully_connected_layers = [1024, 1024]\n",
        "num_of_classes = 1\n",
        "dropout_p = 0.2\n",
        "optimizer = 'adam'\n",
        "loss = 'binary_crossentropy'\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "tf.keras.layers.Embedding(vocab_size + 1,\n",
        "                            5,\n",
        "                            input_length=input_size),\n",
        "tf.keras.layers.Conv1D(256, 5, activation='relu'),\n",
        "tf.keras.layers.MaxPooling1D(pool_size=4),\n",
        "tf.keras.layers.Conv1D(256, 5, activation='relu'),\n",
        "tf.keras.layers.MaxPooling1D(pool_size=4),\n",
        "tf.keras.layers.Flatten(),\n",
        "tf.keras.layers.Dropout(0.2),\n",
        "tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])  # Adam, categorical_crossentropy\n",
        "model.summary()\n",
        "\n",
        "# Shuffle\n",
        "\n",
        "\n",
        "x_train = train_data\n",
        "y_train = training_labels\n",
        "\n",
        "x_test = test_data\n",
        "y_test = test_labels\n",
        "\n",
        "# Training\n",
        "history4=model.fit(x_train, y_train,\n",
        "          validation_data=(x_test, y_test), \n",
        "          batch_size=128,\n",
        "          epochs=10,\n",
        "          verbose=1)  \n",
        "newcorpus = []\n",
        "newsentences=[]\n",
        "newlabels=[]\n",
        "with open('/content/totest.csv',encoding='cp1252') as csvfile:\n",
        "    newreader = csv.reader(csvfile, delimiter=',')\n",
        "    for row in newreader:\n",
        "        list_item=[]\n",
        "        list_item.append(row[5])\n",
        "        this_label=row[0]\n",
        "        if this_label=='0':\n",
        "            list_item.append(0)\n",
        "        else:\n",
        "            list_item.append(1)\n",
        "        num_sentences = num_sentences + 1\n",
        "        newcorpus.append(list_item)     \n",
        "random.shuffle(newcorpus)\n",
        "sentences=[]\n",
        "labels=[]\n",
        "num_sentences = 0\n",
        "\n",
        "\n",
        "for x in range(s):\n",
        "    newsentences.append(newcorpus[x][0])\n",
        "    newlabels.append(newcorpus[x][1])\n",
        "\n",
        "\n",
        "test_sequences = newsentences[0:498]\n",
        "teest_labels = newlabels[0:498]  \n",
        "teest_texts = test_sequences\n",
        "teest_texts = [s.lower() for s in teest_texts]\n",
        "\n",
        "\n",
        "\n",
        "# =======================Convert string to index================\n",
        "# Tokenizer\n",
        "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "tk.fit_on_texts(teest_texts)\n",
        "# If we already have a character list, then replace the tk.word_index\n",
        "# If not, just skip below part\n",
        "\n",
        "# -----------------------Skip part start--------------------------\n",
        "# construct a new vocabulary\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "\n",
        "# Use char_dict to replace the tk.word_index\n",
        "tk.word_index = char_dict.copy()\n",
        "# Add 'UNK' to the vocabulary\n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
        "# -----------------------Skip part end----------------------------\n",
        "\n",
        "# Convert string to index\n",
        "teest_seq = tk.texts_to_sequences(teest_texts)\n",
        "\n",
        "# Padding\n",
        "teest_data = pad_sequences(teest_seq, maxlen=1014, padding='post')\n",
        "\n",
        "# Convert to numpy array\n",
        "teest_data = np.array(teest_data, dtype='float32')\n",
        "result4 = model.evaluate(teest_data, teest_labels, batch_size=128)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fptr8S8Jr6V7",
        "colab_type": "text"
      },
      "source": [
        "# Results for STS in Tabular form\n",
        "---\n",
        "Results for STS corpus\n",
        "\n",
        "\n",
        "Model | Accuracy(unsupervised pre-training) | Accuracy( random word embeddings)|\n",
        "--- | --- | --- \n",
        "CharSCNN     | 76% | 72% \n",
        "SCNN | 79.5% | 70.6%\n",
        "CharSCNN (Santos and Gatti, 2014)     | 86.4% | 81.9% \n",
        "SCNN (Santos and Gatti, 2014)   | 85.2% | 82.2%\n",
        "LProp (Speriosu et al., 2011)            | 84.7%\n",
        "MaxEnt (Go et al., 2009)             | 83.0%\n",
        "NB (Go et al., 2009)  | 82.7%\n",
        "SVM (Go et al., 2009) | 82.2%\n",
        "             **Table-2. Accuracy of different models for binary classification using STS.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF8yp4exKaF1",
        "colab_type": "text"
      },
      "source": [
        "# **Reproduction for SSTb dataset**\n",
        "---\n",
        "In order to work with this dataset, firstly we installed pytreebank. Because it contains fine grained (5 classes) sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences. We used pytreebank library to convert tree structured data to a tabular form.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa0z2ttLbVBS",
        "colab_type": "text"
      },
      "source": [
        "# Pretrained SCNN model\n",
        "In the model below, we work with SSTb dataset. It has 8544 samples for training, 1101 samples for validation and 2210 samples test data. Samples contain one of the 5 labels expressing different sentiments. In our model, there are 1 embedding layer, 2 bidirectional lstm layers and 2 dense layers. We chose these layer, because this combination of layers had more accuracy than any other layers we tried. The softmax function is used as activation function, because our data has 5 classes. As optimizer we took adam optimizer from keras with learning rate 0.01. Weights were taken from pretrained glove data with dimension 100. Running the model after constructing it properly, we faced 42% accuracy. Then we tested this model with unused test data, as a result it gave us 40% accuracy. The author of the paper had 48% accuracy for this same model with slightly changes. We can say that our model is good enough. because there are some differences in our and author's models. As pretrained work, he used 2013 december's wikipedia. Unfortunately, we could not use it, because we could not find this exact work, other works weighted several gigabytes in memory. If we compare our model's accuracy  with other popular models', those data can be derived: MVRNN - 44.4%, RNN - 43%, SVM - 40.7%, NB- 41%. So, it can be concluded that our model can be used in the same way as other models. But, We had a powerful computer, we could better our model, by increasing epoch numbers and giving different parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bEiOmX2bmoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytreebank\n",
        "import pytreebank"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP7ctBw8KZbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "out_path = os.path.join(sys.path[0], 'sst_{}.txt')\n",
        "dataset = pytreebank.load_sst('./raw_data')\n",
        "\n",
        "# Store train, dev and test in separate files\n",
        "for category in ['train', 'test', 'dev']:\n",
        "    with open(out_path.format(category), 'w') as outfile:\n",
        "        for item in dataset[category]:\n",
        "            outfile.write(\"{}\\t{}\\n\".format(\n",
        "                item.to_labeled_lines()[0][0] ,\n",
        "                item.to_labeled_lines()[0][1]\n",
        "            ))\n",
        "# Print the length of the training set\n",
        "print(len(dataset['train']))\n",
        "len(dataset['dev'])\n",
        "train = loadtxt(\"sst_train.txt\", dtype=str, comments=\"#\", delimiter=\"\\t\", unpack=False)\n",
        "train_data=pd.DataFrame(train)\n",
        "train_data.columns=['label', 'text']\n",
        "print(train_data)\n",
        "val = loadtxt(\"sst_dev.txt\", dtype=str, comments=\"#\", delimiter=\"\\t\", unpack=False)\n",
        "val_data=pd.DataFrame(val)\n",
        "#val_data.columns=['label', 'text']\n",
        "val_labels=val_data[0]\n",
        "val_labels.shape\n",
        "print(val_labels)\n",
        "print(val_data[0][0])\n",
        "val_sentences=val_data[1]\n",
        "val_sentences.shape\n",
        "print(val_sentences)\n",
        "valid_labels=[]\n",
        "\n",
        "for m in val_labels:\n",
        " valid_labels.append(int(m))\n",
        "  #train_labels.append(str(m))\n",
        "v_labels=np.array(valid_labels)\n",
        "\n",
        "print(v_labels)\n",
        "training_labels=train_data['label']\n",
        "training_labels.shape\n",
        "training_sentences=train_data['text']\n",
        "training_sentences.shape\n",
        "print(training_sentences)\n",
        "train_labels=[]\n",
        "\n",
        "for m in training_labels:\n",
        " train_labels.append(int(m))\n",
        "  #train_labels.append(str(m))\n",
        "tr_labels=np.array(train_labels) \n",
        "print(tr_labels)\n",
        "import re\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence\n",
        "v_sentences=[]\n",
        "\n",
        "for k in val_sentences:\n",
        " v_sentences.append(preprocess_text(k))\n",
        "\n",
        "print(v_sentences)\n",
        "train_sentences=[]\n",
        "\n",
        "for k in training_sentences:\n",
        " train_sentences.append(preprocess_text(k))\n",
        "\n",
        "vocab_size = 100000\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "padded = pad_sequences(sequences, truncating=trunc_type, maxlen=max_length)\n",
        "\n",
        "val_sequences = tokenizer.texts_to_sequences(v_sentences)\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_length)\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "from keras.utils import to_categorical\n",
        "y_binary = to_categorical(v_labels)\n",
        "from keras.utils import to_categorical\n",
        "t_binary = to_categorical(tr_labels)\n",
        "# model = tf.keras.Sequential([\n",
        "#     tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embeddings_matrix], input_length=max_length),\n",
        "#     tf.keras.layers.Conv1D(300, 5, activation='relu'),\n",
        "#     tf.keras.layers.GlobalAveragePooling1D(),\n",
        "#     tf.keras.layers.Dense(6, activation='relu'),\n",
        "#     tf.keras.layers.Dense(5, activation='softmax')\n",
        "# ])\n",
        "# model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "adam = Adam(lr=0.01)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embeddings_matrix],input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n",
        "history5 = model.fit(padded, t_binary, epochs=num_epochs, validation_data=(val_padded, y_binary))\n",
        "test = loadtxt(\"sst_test.txt\", dtype=str, comments=\"#\", delimiter=\"\\t\", unpack=False)\n",
        "test_data=pd.DataFrame(test)\n",
        "#val_data.columns=['label', 'text']\n",
        "testing_labels=test_data[0]\n",
        "print(testing_labels.shape)\n",
        "#print(testing_labels)\n",
        "test_labels=[]\n",
        "\n",
        "for m in testing_labels:\n",
        "  test_labels.append(int(m))\n",
        "t_labels=np.array(test_labels) \n",
        "print(\"For test set :\",t_labels)\n",
        "testing_sentences=test_data[1]\n",
        "testing_sentences.shape\n",
        "print(testing_sentences)\n",
        "test_sentences=[]\n",
        "\n",
        "for k in testing_sentences:\n",
        " test_sentences.append(preprocess_text(k))\n",
        "\n",
        "print(test_sentences)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length)\n",
        "from keras.utils import to_categorical\n",
        "test_binary = to_categorical(t_labels)\n",
        "print(test_binary)\n",
        "results5 = model.evaluate(test_padded, test_binary, batch_size=128)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXRzzskYMZC2",
        "colab_type": "text"
      },
      "source": [
        "# Random SCNN model\n",
        "This SCNN model uses the same elements as pretrained SCNN model, except pretrained work. Here we do not used pretrained weights from any other work. After running the model, we had 35% accuracy. Testing it with unused test data brought 33% accuracy. So, it is clear that pretrained SCNN model works better than this model. We could not compare our results with author's results, because in the paper he did not give information about which SCNN model give the 48% accuracy. We believe that this accuracy was derived from pretrained SCNN model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-CTj8xjNR-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out_path = os.path.join(sys.path[0], 'sst_{}.txt')\n",
        "dataset = pytreebank.load_sst('./raw_data')\n",
        "\n",
        "# Store train, dev and test in separate files\n",
        "for category in ['train', 'test', 'dev']:\n",
        "    with open(out_path.format(category), 'w') as outfile:\n",
        "        for item in dataset[category]:\n",
        "            outfile.write(\"{}\\t{}\\n\".format(\n",
        "                item.to_labeled_lines()[0][0] ,\n",
        "                item.to_labeled_lines()[0][1]\n",
        "            ))\n",
        "# Print the length of the training set\n",
        "print(len(dataset['train']))\n",
        "len(dataset['dev'])\n",
        "train = loadtxt(\"sst_train.txt\", dtype=str, comments=\"#\", delimiter=\"\\t\", unpack=False)\n",
        "train_data=pd.DataFrame(train)\n",
        "train_data.columns=['label', 'text']\n",
        "print(train_data)\n",
        "val = loadtxt(\"sst_dev.txt\", dtype=str, comments=\"#\", delimiter=\"\\t\", unpack=False)\n",
        "val_data=pd.DataFrame(val)\n",
        "#val_data.columns=['label', 'text']\n",
        "val_labels=val_data[0]\n",
        "val_labels.shape\n",
        "print(val_labels)\n",
        "print(val_data[0][0])\n",
        "val_sentences=val_data[1]\n",
        "val_sentences.shape\n",
        "print(val_sentences)\n",
        "valid_labels=[]\n",
        "\n",
        "for m in val_labels:\n",
        " valid_labels.append(int(m))\n",
        "  #train_labels.append(str(m))\n",
        "v_labels=np.array(valid_labels)\n",
        "\n",
        "print(v_labels)\n",
        "training_labels=train_data['label']\n",
        "training_labels.shape\n",
        "training_sentences=train_data['text']\n",
        "training_sentences.shape\n",
        "print(training_sentences)\n",
        "train_labels=[]\n",
        "\n",
        "for m in training_labels:\n",
        " train_labels.append(int(m))\n",
        "  #train_labels.append(str(m))\n",
        "tr_labels=np.array(train_labels) \n",
        "print(tr_labels)\n",
        "import re\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence\n",
        "v_sentences=[]\n",
        "\n",
        "for k in val_sentences:\n",
        " v_sentences.append(preprocess_text(k))\n",
        "\n",
        "print(v_sentences)\n",
        "train_sentences=[]\n",
        "\n",
        "for k in training_sentences:\n",
        " train_sentences.append(preprocess_text(k))\n",
        "\n",
        "vocab_size = 100000\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "padded = pad_sequences(sequences, truncating=trunc_type, maxlen=max_length)\n",
        "\n",
        "val_sequences = tokenizer.texts_to_sequences(v_sentences)\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_length)\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "from keras.utils import to_categorical\n",
        "y_binary = to_categorical(v_labels)\n",
        "from keras.utils import to_categorical\n",
        "t_binary = to_categorical(tr_labels)\n",
        "# model = tf.keras.Sequential([\n",
        "#     tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embeddings_matrix], input_length=max_length),\n",
        "#     tf.keras.layers.Conv1D(300, 5, activation='relu'),\n",
        "#     tf.keras.layers.GlobalAveragePooling1D(),\n",
        "#     tf.keras.layers.Dense(6, activation='relu'),\n",
        "#     tf.keras.layers.Dense(5, activation='softmax')\n",
        "# ])\n",
        "# model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "adam = Adam(lr=0.01)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n",
        "history6 = model.fit(padded, t_binary, epochs=num_epochs, validation_data=(val_padded, y_binary))\n",
        "test = loadtxt(\"sst_test.txt\", dtype=str, comments=\"#\", delimiter=\"\\t\", unpack=False)\n",
        "test_data=pd.DataFrame(test)\n",
        "#val_data.columns=['label', 'text']\n",
        "testing_labels=test_data[0]\n",
        "print(testing_labels.shape)\n",
        "#print(testing_labels)\n",
        "test_labels=[]\n",
        "\n",
        "for m in testing_labels:\n",
        "  test_labels.append(int(m))\n",
        "t_labels=np.array(test_labels) \n",
        "print(\"For test set :\",t_labels)\n",
        "testing_sentences=test_data[1]\n",
        "testing_sentences.shape\n",
        "print(testing_sentences)\n",
        "test_sentences=[]\n",
        "\n",
        "for k in testing_sentences:\n",
        " test_sentences.append(preprocess_text(k))\n",
        "\n",
        "print(test_sentences)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length)\n",
        "from keras.utils import to_categorical\n",
        "test_binary = to_categorical(t_labels)\n",
        "print(test_binary)\n",
        "results6 = model.evaluate(test_padded, test_binary, batch_size=128)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlEq0OsVOMbW",
        "colab_type": "text"
      },
      "source": [
        "# Pretrained CharSCNN model\n",
        "This model consists of sequence of layers such as embedding layer, two one dimensional convolution layers 256 neurons inside, every convolution layer is followed by maxpooling layers with pooling size 4, and flattening layer, dropout layer which drops insignificant elemens, and the last layer is dense layer with 5 neurons. 5 because, we have classes. As optimizer we chose adam, softmax is activation fucntion for categorical case. In this model, we enumerate each existing character with numbers. Hence, the sentences in samples are converted into new array of numbers. It is supposed that this model should bring higher results, since the model deeply analyzes, in other words, we consider every character. As a pretrained work, we use glove samples. By using it, we construct weight matrix. But, when we used char level analysis for STS dataset by using pretrained work, it did not bring higher results. It was explained that glove samples may suit char level analysis. Despite those facts, we run our model and got this results: validation accuracy is 28% and test accuracy is 28% too. If we compare our result with aurthor's accuracy which is 43%, it is clear that he used better pretrained data and got meaningful weights. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvWjwsvROLwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out_path = os.path.join(sys.path[0], 'sst_{}.txt')\n",
        "dataset = pytreebank.load_sst('./raw_data')\n",
        "\n",
        "# Store train, dev and test in separate files\n",
        "for category in ['train', 'test', 'dev']:\n",
        "    with open(out_path.format(category), 'w') as outfile:\n",
        "        for item in dataset[category]:\n",
        "            outfile.write(\"{}\\t{}\\n\".format(\n",
        "                item.to_labeled_lines()[0][0] ,\n",
        "                item.to_labeled_lines()[0][1]\n",
        "            ))\n",
        "# Print the length of the training set\n",
        "print(len(dataset['train']))\n",
        "len(dataset['dev'])\n",
        "train = loadtxt(\"sst_train.txt\", dtype=str, comments=\"#\", delimiter=\"\\t\", unpack=False)\n",
        "train_data=pd.DataFrame(train)\n",
        "train_data.columns=['label', 'text']\n",
        "print(train_data)\n",
        "val = loadtxt(\"sst_dev.txt\", dtype=str, comments=\"#\", delimiter=\"\\t\", unpack=False)\n",
        "val_data=pd.DataFrame(val)\n",
        "#val_data.columns=['label', 'text']\n",
        "val_labels=val_data[0]\n",
        "val_labels.shape\n",
        "print(val_labels)\n",
        "print(val_data[0][0])\n",
        "val_sentences=val_data[1]\n",
        "val_sentences.shape\n",
        "print(val_sentences)\n",
        "valid_labels=[]\n",
        "\n",
        "for m in val_labels:\n",
        " valid_labels.append(int(m))\n",
        "  #train_labels.append(str(m))\n",
        "v_labels=np.array(valid_labels)\n",
        "\n",
        "print(v_labels)\n",
        "training_labels=train_data['label']\n",
        "training_labels.shape\n",
        "training_sentences=train_data['text']\n",
        "training_sentences.shape\n",
        "print(training_sentences)\n",
        "train_labels=[]\n",
        "\n",
        "for m in training_labels:\n",
        " train_labels.append(int(m))\n",
        "  #train_labels.append(str(m))\n",
        "tr_labels=np.array(train_labels) \n",
        "print(tr_labels)\n",
        "import re\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence\n",
        "v_sentences=[]\n",
        "\n",
        "for k in val_sentences:\n",
        " v_sentences.append(preprocess_text(k))\n",
        "\n",
        "print(v_sentences)\n",
        "train_sentences=[]\n",
        "\n",
        "for k in training_sentences:\n",
        " train_sentences.append(preprocess_text(k))\n",
        "\n",
        "print(train_sentences)\n",
        "train_texts = train_sentences\n",
        "train_texts = [s.lower() for s in train_texts]\n",
        "\n",
        "\n",
        "test_texts = v_sentences\n",
        "test_texts = [s.lower() for s in test_texts]\n",
        "\n",
        "# =======================Convert string to index================\n",
        "# Tokenizer\n",
        "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "tk.fit_on_texts(train_texts)\n",
        "# If we already have a character list, then replace the tk.word_index\n",
        "# If not, just skip below part\n",
        "\n",
        "# -----------------------Skip part start--------------------------\n",
        "# construct a new vocabulary\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "\n",
        "# Use char_dict to replace the tk.word_index\n",
        "tk.word_index = char_dict.copy()\n",
        "# Add 'UNK' to the vocabulary\n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
        "# -----------------------Skip part end----------------------------\n",
        "\n",
        "# Convert string to index\n",
        "train_seq = tk.texts_to_sequences(train_texts)\n",
        "test_texts = tk.texts_to_sequences(test_texts)\n",
        "\n",
        "\n",
        "# Padding\n",
        "train_data = pad_sequences(train_seq, maxlen=1014, padding='post')\n",
        "test_data = pad_sequences(test_texts, maxlen=1014, padding='post')\n",
        "\n",
        "# Convert to numpy array\n",
        "train_data = np.array(train_data, dtype='float32')\n",
        "test_data = np.array(test_data, dtype='float32')\n",
        "word_index = tk.word_index\n",
        "from keras.utils import to_categorical\n",
        "y_binary = to_categorical(v_labels)\n",
        "from keras.utils import to_categorical\n",
        "t_binary = to_categorical(tr_labels)\n",
        "import tensorflow as tf\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embeddings_matrix], input_length=max_length),\n",
        "    tf.keras.layers.Conv1D(300, 5, activation='relu'),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n",
        "history7 = model.fit(train_data, t_binary, epochs=num_epochs, validation_data=(test_data, y_binary))\n",
        "test = loadtxt(\"sst_test.txt\", dtype=str, comments=\"#\", delimiter=\"\\t\", unpack=False)\n",
        "test_data=pd.DataFrame(test)\n",
        "#val_data.columns=['label', 'text']\n",
        "testing_labels=test_data[0]\n",
        "print(testing_labels.shape)\n",
        "#print(testing_labels)\n",
        "test_labels=[]\n",
        "\n",
        "for m in testing_labels:\n",
        "  test_labels.append(int(m))\n",
        "t_labels=np.array(test_labels) \n",
        "print(\"For test set :\",t_labels)\n",
        "testing_sentences=test_data[1]\n",
        "testing_sentences.shape\n",
        "print(testing_sentences)\n",
        "test_sentences=[]\n",
        "for k in testing_sentences:\n",
        " test_sentences.append(preprocess_text(k))\n",
        "\n",
        "\n",
        "teest_texts = test_sentences\n",
        "teest_texts = [s.lower() for s in teest_texts]\n",
        "\n",
        "\n",
        "\n",
        "# =======================Convert string to index================\n",
        "# Tokenizer\n",
        "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "tk.fit_on_texts(teest_texts)\n",
        "# If we already have a character list, then replace the tk.word_index\n",
        "# If not, just skip below part\n",
        "\n",
        "# -----------------------Skip part start--------------------------\n",
        "# construct a new vocabulary\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "\n",
        "# Use char_dict to replace the tk.word_index\n",
        "tk.word_index = char_dict.copy()\n",
        "# Add 'UNK' to the vocabulary\n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
        "# -----------------------Skip part end----------------------------\n",
        "\n",
        "# Convert string to index\n",
        "teest_seq = tk.texts_to_sequences(teest_texts)\n",
        "\n",
        "# Padding\n",
        "teest_data = pad_sequences(teest_seq, maxlen=1014, padding='post')\n",
        "\n",
        "# Convert to numpy array\n",
        "teest_data = np.array(teest_data, dtype='float32')\n",
        "from keras.utils import to_categorical\n",
        "test_binary = to_categorical(t_labels)\n",
        "print(test_binary)\n",
        "results7 = model.evaluate(teest_data, test_binary, batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn6s-JMOQIKu",
        "colab_type": "text"
      },
      "source": [
        "# Random Char SCNN model\n",
        "\n",
        "This model consists of the same layers as previous model, but the difference is we don't we pretrained data which give us weights as a matrix. Usually, when pretrained data does not suit the model you use, in our case character level CNN, it is better to not use it. For example, our model without glove bring us 30% accuracy which is higher than previous model's accuracy. Testing it with unused test data resulted in 28% accuracy which is same as in Pretrained Char SCNN model. If we compare it with author's accuracu which is 43%, it is clear that our model works poorly. But there are several enhancements can be made to increase the accuracy. Increasing the epoch number, finding good pretrained source, changing parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWYfEHsvQH1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out_path = os.path.join(sys.path[0], 'sst_{}.txt')\n",
        "dataset = pytreebank.load_sst('./raw_data')\n",
        "\n",
        "# Store train, dev and test in separate files\n",
        "for category in ['train', 'test', 'dev']:\n",
        "    with open(out_path.format(category), 'w') as outfile:\n",
        "        for item in dataset[category]:\n",
        "            outfile.write(\"{}\\t{}\\n\".format(\n",
        "                item.to_labeled_lines()[0][0] ,\n",
        "                item.to_labeled_lines()[0][1]\n",
        "            ))\n",
        "# Print the length of the training set\n",
        "print(len(dataset['train']))\n",
        "len(dataset['dev'])\n",
        "train = loadtxt(\"sst_train.txt\", dtype=str, comments=\"#\", delimiter=\"\\t\", unpack=False)\n",
        "train_data=pd.DataFrame(train)\n",
        "train_data.columns=['label', 'text']\n",
        "print(train_data)\n",
        "val = loadtxt(\"sst_dev.txt\", dtype=str, comments=\"#\", delimiter=\"\\t\", unpack=False)\n",
        "val_data=pd.DataFrame(val)\n",
        "#val_data.columns=['label', 'text']\n",
        "val_labels=val_data[0]\n",
        "val_labels.shape\n",
        "print(val_labels)\n",
        "print(val_data[0][0])\n",
        "val_sentences=val_data[1]\n",
        "val_sentences.shape\n",
        "print(val_sentences)\n",
        "valid_labels=[]\n",
        "\n",
        "for m in val_labels:\n",
        " valid_labels.append(int(m))\n",
        "  #train_labels.append(str(m))\n",
        "v_labels=np.array(valid_labels)\n",
        "\n",
        "print(v_labels)\n",
        "training_labels=train_data['label']\n",
        "training_labels.shape\n",
        "training_sentences=train_data['text']\n",
        "training_sentences.shape\n",
        "print(training_sentences)\n",
        "train_labels=[]\n",
        "\n",
        "for m in training_labels:\n",
        " train_labels.append(int(m))\n",
        "  #train_labels.append(str(m))\n",
        "tr_labels=np.array(train_labels) \n",
        "print(tr_labels)\n",
        "import re\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence\n",
        "v_sentences=[]\n",
        "\n",
        "for k in val_sentences:\n",
        " v_sentences.append(preprocess_text(k))\n",
        "\n",
        "print(v_sentences)\n",
        "train_sentences=[]\n",
        "\n",
        "for k in training_sentences:\n",
        " train_sentences.append(preprocess_text(k))\n",
        "\n",
        "print(train_sentences)\n",
        "train_texts = train_sentences\n",
        "train_texts = [s.lower() for s in train_texts]\n",
        "\n",
        "\n",
        "test_texts = v_sentences\n",
        "test_texts = [s.lower() for s in test_texts]\n",
        "\n",
        "# =======================Convert string to index================\n",
        "# Tokenizer\n",
        "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "tk.fit_on_texts(train_texts)\n",
        "# If we already have a character list, then replace the tk.word_index\n",
        "# If not, just skip below part\n",
        "\n",
        "# -----------------------Skip part start--------------------------\n",
        "# construct a new vocabulary\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "\n",
        "# Use char_dict to replace the tk.word_index\n",
        "tk.word_index = char_dict.copy()\n",
        "# Add 'UNK' to the vocabulary\n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
        "# -----------------------Skip part end----------------------------\n",
        "\n",
        "# Convert string to index\n",
        "train_seq = tk.texts_to_sequences(train_texts)\n",
        "test_texts = tk.texts_to_sequences(test_texts)\n",
        "\n",
        "\n",
        "# Padding\n",
        "train_data = pad_sequences(train_seq, maxlen=1014, padding='post')\n",
        "test_data = pad_sequences(test_texts, maxlen=1014, padding='post')\n",
        "\n",
        "# Convert to numpy array\n",
        "train_data = np.array(train_data, dtype='float32')\n",
        "test_data = np.array(test_data, dtype='float32')\n",
        "word_index = tk.word_index\n",
        "from keras.utils import to_categorical\n",
        "y_binary = to_categorical(v_labels)\n",
        "from keras.utils import to_categorical\n",
        "t_binary = to_categorical(tr_labels)\n",
        "import tensorflow as tf\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Conv1D(300, 5, activation='relu'),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n",
        "history7 = model.fit(train_data, t_binary, epochs=num_epochs, validation_data=(test_data, y_binary))\n",
        "test = loadtxt(\"sst_test.txt\", dtype=str, comments=\"#\", delimiter=\"\\t\", unpack=False)\n",
        "test_data=pd.DataFrame(test)\n",
        "#val_data.columns=['label', 'text']\n",
        "testing_labels=test_data[0]\n",
        "print(testing_labels.shape)\n",
        "#print(testing_labels)\n",
        "test_labels=[]\n",
        "\n",
        "for m in testing_labels:\n",
        "  test_labels.append(int(m))\n",
        "t_labels=np.array(test_labels) \n",
        "print(\"For test set :\",t_labels)\n",
        "testing_sentences=test_data[1]\n",
        "testing_sentences.shape\n",
        "print(testing_sentences)\n",
        "test_sentences=[]\n",
        "for k in testing_sentences:\n",
        " test_sentences.append(preprocess_text(k))\n",
        "\n",
        "\n",
        "teest_texts = test_sentences\n",
        "teest_texts = [s.lower() for s in teest_texts]\n",
        "\n",
        "\n",
        "\n",
        "# =======================Convert string to index================\n",
        "# Tokenizer\n",
        "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
        "tk.fit_on_texts(teest_texts)\n",
        "# If we already have a character list, then replace the tk.word_index\n",
        "# If not, just skip below part\n",
        "\n",
        "# -----------------------Skip part start--------------------------\n",
        "# construct a new vocabulary\n",
        "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1\n",
        "\n",
        "# Use char_dict to replace the tk.word_index\n",
        "tk.word_index = char_dict.copy()\n",
        "# Add 'UNK' to the vocabulary\n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
        "# -----------------------Skip part end----------------------------\n",
        "\n",
        "# Convert string to index\n",
        "teest_seq = tk.texts_to_sequences(teest_texts)\n",
        "\n",
        "# Padding\n",
        "teest_data = pad_sequences(teest_seq, maxlen=1014, padding='post')\n",
        "\n",
        "# Convert to numpy array\n",
        "teest_data = np.array(teest_data, dtype='float32')\n",
        "from keras.utils import to_categorical\n",
        "test_binary = to_categorical(t_labels)\n",
        "print(test_binary)\n",
        "results8 = model.evaluate(teest_data, test_binary, batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p55owolnx_lZ",
        "colab_type": "text"
      },
      "source": [
        "# **Results for SSTb in Tabular form**\n",
        "---\n",
        "Results for SSTb corpus\n",
        "\n",
        "\n",
        "Model | Accuracy(unsupervised pre-training) | Accuracy( random word embeddings)|\n",
        "--- | --- | --- \n",
        "CharSCNN     | 28% | 28% \n",
        "SCNN | 40% | 33%\n",
        "CharSCNN (Santos and Gatti, 2014)     | 43%\n",
        "SCNN (Santos and Gatti, 2014)   | 48% \n",
        "RNTN (Socher et al., 2013b)            | 45.7%\n",
        "MV-RNN (Socher et al., 2013b)              | 44.4%\n",
        "RNN (Socher et al., 2013b)   | 43.2%\n",
        "NB (Socher et al., 2013b)  | 41.0%\n",
        "SVM (Socher et al., 2013b)  | 40.7%\n",
        "             **Table-3. Accuracy of different models for binary classification using SSTb.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8hLJP31Qs51",
        "colab_type": "text"
      },
      "source": [
        "# **Conclusion**\n",
        "---\n",
        "In this work we tried to reproduce the paper \"Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts\", which is done by Santos and Gatti in 2014. We have used CharSCNN, SCNN  for sentiment analysis of STS and SSTb datasets. Both models were used with and without pretrained data. As a result, we run 8 models, 4 for each dataset. We came to conclusion that SCNN model works best for STS dataset, because this model brought the highest result. However, every model used for STS dataset can be improved by using more epochs and different parameters. When it comes to SSTb dataset, similarly, pretrained SCNN model reached the best accuracy which close enough to the author had. All in all, after reproducing the paper, we were convinced that NLP can be applied to analyze human sentiments. Even if no one has 100% accuracy, every model made in the past can be improved, to reach more accuracy."
      ]
    }
  ]
}